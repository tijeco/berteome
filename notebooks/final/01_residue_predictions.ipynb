{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp berteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import BertTokenizer, BertForMaskedLM, EsmTokenizer, EsmForMaskedLM\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class modelPredDF():\n",
    "    def __init__(self, seq, tokenizer, model):\n",
    "        self.aas = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "        predDict = self.predictionDict(seq, tokenizer, model)\n",
    "        self.predDf = pd.DataFrame.from_dict(predDict, orient = \"index\", columns = list(self.aas))\n",
    "        self.predDf = self.predDf.div(self.predDf.sum(axis=1),axis=0)\n",
    "        self.predDf.insert(0, \"wt\",list(seq))\n",
    "        self.predDf.insert(1, \"wtIndex\",list(range(1,len(seq)+1)))\n",
    "        wtScore = self.wtScoreCol()\n",
    "        self.predDf.insert(2, \"wtScore\",wtScore)\n",
    "    \n",
    "    def predictionDict(self, seq, tokenizer, model):\n",
    "      naturalAAIndices = naturalAAIndex(self.aas,tokenizer)\n",
    "      predDict = {}\n",
    "      for wtIndex in range(len(seq)):\n",
    "        maskedSeq = tokenizeSeq(seq, tokenizer, mask_index = wtIndex)\n",
    "        seq_logits = run_model(model, maskedSeq)\n",
    "        seq_probs = logits2prob(seq_logits)\n",
    "        predDict[wtIndex] = [i.item() for i in getNatProbs(naturalAAIndices, seq_probs[0, wtIndex +1])]\n",
    "      #predDF = modelPredDF(predDict, seq, aas).predDf\n",
    "      return predDict\n",
    "    \n",
    "    def wtScoreCol(self):\n",
    "      wtScore = []\n",
    "      for row in self.predDf.to_dict(orient=\"records\"):\n",
    "        wt = row[\"wt\"]\n",
    "        wtScore.append(row[wt])\n",
    "      return wtScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class modelLoader():\n",
    "  def __init__(self):\n",
    "    self.supported_model_dict = {\n",
    "        \"Rostlab/prot_bert\" : self.token_model_dict(\"prot_bert\"),\n",
    "        \"facebook/esm2_t33_650M_UR50D\" : self.token_model_dict(\"esm\"),\n",
    "        \"facebook/esm1b_t33_650M_UR50S\": self.token_model_dict(\"esm\")\n",
    "    }\n",
    "    self.supported_models = list(self.supported_model_dict.keys())\n",
    "\n",
    "  \n",
    "  def token_model_dict(self, model_name):\n",
    "    if model_name == \"prot_bert\":\n",
    "      tokenModelDict = {\"tokenizer\":BertTokenizer, \"model\":BertForMaskedLM}\n",
    "    if model_name == \"esm\":\n",
    "      tokenModelDict = {\"tokenizer\":EsmTokenizer, \"model\":EsmForMaskedLM}\n",
    "    return tokenModelDict\n",
    "  \n",
    "  def load_model(self, model_path):\n",
    "    tokenizerLM = self.supported_model_dict[model_path][\"tokenizer\"]\n",
    "    maskedLM = self.supported_model_dict[model_path][\"model\"]\n",
    "    tokenizer = tokenizerLM.from_pretrained(model_path)\n",
    "    model = maskedLM.from_pretrained(model_path)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def run_model(model, inputs):\n",
    "  with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "  return logits\n",
    "\n",
    "def logits2prob(logits):\n",
    "  return torch.softmax(logits,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def maskifySeq(seq, tokenizer, i):\n",
    "    seqList = list(seq)\n",
    "    if i != None:\n",
    "      seqList[i] = tokenizer.mask_token \n",
    "    return \" \".join(seqList)\n",
    "\n",
    "def tokenizeSeq(seq, tokenizer, mask_index = None, return_tensors = \"pt\"):\n",
    "  maskified_seq = maskifySeq(seq, tokenizer, mask_index)\n",
    "  return tokenizer(maskified_seq, return_tensors=return_tensors)\n",
    "\n",
    "def naturalAAIndex(aas, tokenizer):\n",
    "    return tokenizeSeq(aas, tokenizer, return_tensors=None)[\"input_ids\"][1:-1]\n",
    "\n",
    "def getNatProbs(natAAList,probList):\n",
    "    natProbList = []\n",
    "    for natAAIndex in natAAList:\n",
    "      natProbList.append(probList[natAAIndex])\n",
    "    return natProbList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
